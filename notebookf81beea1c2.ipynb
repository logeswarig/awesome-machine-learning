{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5403505,"sourceType":"datasetVersion","datasetId":3130749},{"sourceId":8224503,"sourceType":"datasetVersion","datasetId":4876317}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/logeswarig/notebookf81beea1c2?scriptVersionId=283165016\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:09:16.425634Z","iopub.execute_input":"2025-11-20T03:09:16.42591Z","iopub.status.idle":"2025-11-20T03:09:28.303095Z","shell.execute_reply.started":"2025-11-20T03:09:16.425887Z","shell.execute_reply":"2025-11-20T03:09:28.301788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nPOTATO_PATH = \"/kaggle/input/plantvillage-potato-disease-dataset\"\nMANGO_PATH = \"/kaggle/input/mango-leaf-disease-dataset\"\n\nprint(\"Potato path:\", POTATO_PATH)\nprint(\"Mango path:\", MANGO_PATH)\ndef load_dataset(path):\n    images = []\n    labels = []\n\n    for label in os.listdir(path):\n        class_dir = os.path.join(path, label)\n\n        if os.path.isdir(class_dir):  # Only process folders\n            for img_name in os.listdir(class_dir):\n                img_path = os.path.join(class_dir, img_name)\n\n                try:\n                    img = cv2.imread(img_path)\n                    if img is not None:\n                        img = cv2.resize(img, (128, 128))\n                        images.append(img)\n                        labels.append(label)\n                except:\n                    pass\n\n    return np.array(images), np.array(labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:20:47.045787Z","iopub.execute_input":"2025-11-20T03:20:47.046135Z","iopub.status.idle":"2025-11-20T03:20:47.053872Z","shell.execute_reply.started":"2025-11-20T03:20:47.046109Z","shell.execute_reply":"2025-11-20T03:20:47.052722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"potato_images, potato_labels = load_dataset(POTATO_PATH)\nmango_images, mango_labels = load_dataset(MANGO_PATH)\n\nprint(\"Potato dataset loaded:\", potato_images.shape, potato_labels.shape)\nprint(\"Mango dataset loaded:\", mango_images.shape, mango_labels.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:20:58.423291Z","iopub.execute_input":"2025-11-20T03:20:58.423613Z","iopub.status.idle":"2025-11-20T03:21:21.817193Z","shell.execute_reply.started":"2025-11-20T03:20:58.42359Z","shell.execute_reply":"2025-11-20T03:21:21.816349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(\"Potato Dataset Structure:\")\nfor root, dirs, files in os.walk(\"/kaggle/input/plantvillage-potato-disease-dataset\"):\n    print(root, \"| Dirs:\", dirs, \"| Files:\", files[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:22:56.598374Z","iopub.execute_input":"2025-11-20T03:22:56.599443Z","iopub.status.idle":"2025-11-20T03:22:57.024808Z","shell.execute_reply.started":"2025-11-20T03:22:56.599408Z","shell.execute_reply":"2025-11-20T03:22:57.023881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# ---------------------------------------------------\n# Correct Dataset Paths\n# ---------------------------------------------------\nPOTATO_PATH = \"/kaggle/input/plantvillage-potato-disease-dataset/PlantVillage\"\nMANGO_PATH = \"/kaggle/input/mango-leaf-disease-dataset\"\n\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\n\n# ---------------------------------------------------\n# Function to Load Dataset\n# ---------------------------------------------------\ndef load_dataset(path):\n    images = []\n    labels = []\n\n    for label in os.listdir(path):\n        class_dir = os.path.join(path, label)\n        if not os.path.isdir(class_dir):\n            continue\n        \n        print(\"Loading:\", label)\n\n        for img_name in tqdm(os.listdir(class_dir)):\n            img_path = os.path.join(class_dir, img_name)\n\n            try:\n                img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n                img = img_to_array(img)\n\n                # Normalization (0 to 1)\n                img = img / 255.0\n\n                images.append(img)\n                labels.append(label)\n            except:\n                pass\n\n    return np.array(images), np.array(labels)\n\n# ---------------------------------------------------\n# Load Both Datasets\n# ---------------------------------------------------\npotato_images, potato_labels = load_dataset(POTATO_PATH)\nmango_images, mango_labels = load_dataset(MANGO_PATH)\n\nprint(\"Potato dataset loaded:\", potato_images.shape, potato_labels.shape)\nprint(\"Mango dataset loaded:\", mango_images.shape, mango_labels.shape)\n\n# ---------------------------------------------------\n# Combine Datasets\n# ---------------------------------------------------\nX = np.concatenate([potato_images, mango_images], axis=0)\ny = np.concatenate([potato_labels, mango_labels], axis=0)\n\nprint(\"\\nFinal Combined Dataset:\", X.shape, y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:23:48.424075Z","iopub.execute_input":"2025-11-20T03:23:48.424448Z","iopub.status.idle":"2025-11-20T03:24:11.979132Z","shell.execute_reply.started":"2025-11-20T03:23:48.42442Z","shell.execute_reply":"2025-11-20T03:24:11.978138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# 1. IMPORT LIBRARIES\n# ============================================\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n\n# ============================================\n# 2. DATASET PATHS\n# ============================================\nPOTATO_PATH = \"/kaggle/input/plantvillage-potato-disease-dataset/PlantVillage\"\nMANGO_PATH = \"/kaggle/input/mango-leaf-disease-dataset\"\n\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\n\n# ============================================\n# 3. FUNCTION TO LOAD + RESIZE + NORMALIZE IMAGES\n# ============================================\ndef load_dataset(path):\n    images = []\n    labels = []\n\n    for label in os.listdir(path):\n        class_dir = os.path.join(path, label)\n        if not os.path.isdir(class_dir):\n            continue\n        \n        print(\"Loading:\", label)\n\n        for img_name in tqdm(os.listdir(class_dir)):\n            img_path = os.path.join(class_dir, img_name)\n\n            try:\n                # Load + Resize\n                img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n                img = img_to_array(img)\n\n                # Pixel Normalization (Equation 1)\n                img = (img - img.min()) / (img.max() - img.min())\n\n                images.append(img)\n                labels.append(label)\n\n            except:\n                pass\n\n    return np.array(images), np.array(labels)\n\n# ============================================\n# 4. LOAD BOTH DATASETS\n# ============================================\npotato_images, potato_labels = load_dataset(POTATO_PATH)\nmango_images, mango_labels = load_dataset(MANGO_PATH)\n\nprint(\"\\nPotato dataset:\", potato_images.shape, potato_labels.shape)\nprint(\"Mango dataset:\", mango_images.shape, mango_labels.shape)\n\n# ============================================\n# 5. MERGE DATASETS\n# ============================================\nX = np.concatenate([potato_images, mango_images], axis=0)\ny = np.concatenate([potato_labels, mango_labels], axis=0)\n\nprint(\"\\nCombined Dataset:\", X.shape, y.shape)\n\n# ============================================\n# 6. LABEL ENCODING (String → Integer)\n# ============================================\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)\n\nprint(\"\\nClasses:\", encoder.classes_)\nprint(\"Encoded labels example:\", y_encoded[:10])\n\n# ============================================\n# 7. DATA AUGMENTATION\n# ============================================\naugmentor = ImageDataGenerator(\n    rotation_range=20,\n    horizontal_flip=True,\n    vertical_flip=True,\n    zoom_range=0.2,\n    shear_range=0.2,\n    brightness_range=(0.7, 1.3),\n    fill_mode='nearest'\n)\n\n# ============================================\n# 8. TRAIN–TEST SPLIT (80:20) — Stratified\n# ============================================\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_encoded, test_size=0.20, stratify=y_encoded, random_state=42\n)\n\nprint(\"\\nTraining samples:\", X_train.shape[0])\nprint(\"Testing samples:\", X_test.shape[0])\n\n# ============================================\n# 9. VALIDATION SPLIT FROM TRAINING SET (10%)\n# ============================================\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.10, stratify=y_train, random_state=42\n)\n\nprint(\"\\nFinal Train:\", X_train.shape)\nprint(\"Validation:\", X_val.shape)\nprint(\"Test:\", X_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:26:24.848037Z","iopub.execute_input":"2025-11-20T03:26:24.848385Z","iopub.status.idle":"2025-11-20T03:26:47.200655Z","shell.execute_reply.started":"2025-11-20T03:26:24.848357Z","shell.execute_reply":"2025-11-20T03:26:47.199658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# 1. IMPORT LIBRARIES\n# ============================================\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n\n# ============================================\n# 2. BASELINE CNN MODEL (With Dropout = 0.4)\n# ============================================\ndef build_baseline_cnn(input_shape=(224, 224, 3), num_classes=2, dropout_rate=0.4):\n\n    model = Sequential()\n\n    # ---- Convolution Block 1 ----\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same',\n                     kernel_initializer='he_normal', input_shape=input_shape))\n    model.add(MaxPooling2D((2, 2)))\n\n    # ---- Convolution Block 2 ----\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same',\n                     kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2)))\n\n    # ---- Convolution Block 3 ----\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same',\n                     kernel_initializer='he_normal'))\n    model.add(MaxPooling2D((2, 2)))\n\n    # ---- Flatten ----\n    model.add(Flatten())\n\n    # ---- Dense Layer + Dropout (0.4) ----\n    model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n    model.add(Dropout(dropout_rate))\n\n    # ---- Output Layer ----\n    model.add(Dense(num_classes, activation='softmax'))\n\n    return model\n\n\n# ============================================\n# 3. COMPILE MODEL WITH ADAM (LR = 0.0001)\n# ============================================\ndef compile_model(model, learning_rate=0.0001):\n\n    optimizer = Adam(learning_rate=learning_rate)\n\n    model.compile(\n        optimizer=optimizer,\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n\n# ============================================\n# 4. TRAINING FUNCTION WITH\n#    • Early Stopping (patience=5)\n#    • LR Scheduler (ReduceLROnPlateau)\n# ============================================\ndef train_model(model, X_train, y_train, X_val, y_val,\n                epochs=30, batch_size=32):\n\n    # Early Stopping\n    early_stop = EarlyStopping(\n        monitor=\"val_loss\",\n        patience=5,\n        restore_best_weights=True\n    )\n\n    # Learning Rate Scheduler\n    lr_scheduler = ReduceLROnPlateau(\n        monitor=\"val_loss\",\n        factor=0.2,            # Reduce LR by factor\n        patience=2,            # If no improvement for 2 epochs\n        min_lr=1e-7,\n        verbose=1\n    )\n\n    # Training\n    history = model.fit(\n        X_train, y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(X_val, y_val),\n        callbacks=[early_stop, lr_scheduler]\n    )\n\n    return history\n\n\n# ============================================\n# 5. EVALUATION FUNCTION\n# ============================================\ndef evaluate_model(model, X_test, y_test):\n    loss, acc = model.evaluate(X_test, y_test, verbose=1)\n    print(f\"\\nTest Accuracy: {acc * 100:.2f}%\")\n    print(f\"Test Loss: {loss:.4f}\")\n    return loss, acc\n\n\n# ============================================\n# 6. HOW TO USE\n# ============================================\n# num_classes = len(np.unique(y_train))\n\n# model = build_baseline_cnn(input_shape=(224,224,3), num_classes=num_classes)\n# model = compile_model(model, learning_rate=0.0001)\n# history = train_model(model, X_train, y_train, X_val, y_val)\n# evaluate_model(model, X_test, y_test)\n# ============== RUN MODEL TRAINING ==============\n\nnum_classes = len(np.unique(y_train))\n\nmodel = build_baseline_cnn(\n    input_shape=(224, 224, 3),\n    num_classes=num_classes,\n    dropout_rate=0.4\n)\n\nmodel = compile_model(model, learning_rate=0.0001)\n\nprint(\"\\nTraining Started...\\n\")\n\nhistory = train_model(\n    model,\n    X_train, y_train,\n    X_val, y_val,\n    epochs=30,\n    batch_size=32\n)\n\nprint(\"\\nTraining Completed!\\n\")\n\n# ============== RUN MODEL EVALUATION ==============\n\nevaluate_model(model, X_test, y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:53:08.485116Z","iopub.execute_input":"2025-11-20T03:53:08.485443Z","iopub.status.idle":"2025-11-20T06:27:01.379306Z","shell.execute_reply.started":"2025-11-20T03:53:08.485418Z","shell.execute_reply":"2025-11-20T06:27:01.378193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}